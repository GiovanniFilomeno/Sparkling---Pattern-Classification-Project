{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ddabec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained checkpoint:  ATST-F_strong_1\n",
      "modello caricato con successo\n",
      "Comincio il training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.6948, val_loss=0.6933 | F1_micro=0.0084 | mAP@5=0.0081 | mAP@10=0.0105\n",
      "🏆 New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train_loss=0.6932, val_loss=0.6932 | F1_micro=0.0084 | mAP@5=0.0070 | mAP@10=0.0099\n",
      "🏆 New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train_loss=0.6932, val_loss=0.6932 | F1_micro=0.0084 | mAP@5=0.0061 | mAP@10=0.0096\n",
      "🏆 New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    122\u001b[39m model.train()\n\u001b[32m    123\u001b[39m total_tr = \u001b[32m0.\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m02d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m [Train]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/torch/utils/data/dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mSegmentDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     X = torch.tensor(data[\u001b[33m\"\u001b[39m\u001b[33mlogmel\u001b[39m\u001b[33m\"\u001b[39m], dtype=torch.float32).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     48\u001b[39m     y = torch.tensor(data[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m], dtype=torch.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:451\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    449\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# # %% ---------------------\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics import f1_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import sys\n",
    "\n",
    "# # Aggiungi la cartella che contiene \"PretrainedSED\" al PYTHONPATH\n",
    "# sys.path.insert(0, \"/Users/Q540900/Desktop/Sparkling---Pattern-Classification-Project/04 - Model Training/Bonus\")\n",
    "\n",
    "# from PretrainedSED.models.atstframe.ATSTF_wrapper import ATSTWrapper\n",
    "\n",
    "# import os\n",
    "# os.makedirs(\"resources\", exist_ok=True)\n",
    "# # os.symlink(\"PretrainedSED/resources/ATST-F_strong_1.pt\", \"resources/ATST-F_strong_1.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# # %% ---------------------\n",
    "# SEGMENTS_DIR = Path(\"data/segments\")\n",
    "# BATCH_SIZE = 32\n",
    "# EPOCHS = 50\n",
    "# LEARNING_RATE = 1e-5\n",
    "# NUM_CLASSES = 10\n",
    "# DEVICE = torch.device(\"mps\")\n",
    "# PATIENCE = 5\n",
    "\n",
    "# CLASSES = ['Speech', 'Dog Bark', 'Rooster Crow', 'Shout',\n",
    "#            'Lawn Mower', 'Chainsaw', 'Jackhammer',\n",
    "#            'Power Drill', 'Horn Honk', 'Siren']\n",
    "\n",
    "# # %% ---------------------\n",
    "# class SegmentDataset(Dataset):\n",
    "#     def __init__(self, folder):\n",
    "#         self.files = list(Path(folder).glob(\"*.npz\"))\n",
    "#     def __len__(self):\n",
    "#         return len(self.files)\n",
    "#     def __getitem__(self, idx):\n",
    "#         data = np.load(self.files[idx])\n",
    "#         X = torch.tensor(data[\"logmel\"], dtype=torch.float32).unsqueeze(0)\n",
    "#         y = torch.tensor(data[\"labels\"], dtype=torch.float32)\n",
    "#         return X, y\n",
    "\n",
    "# # %% ---------------------\n",
    "# dataset = SegmentDataset(SEGMENTS_DIR)\n",
    "# n = len(dataset)\n",
    "# n_valid = int(0.2 * n)\n",
    "# n_train = n - n_valid\n",
    "# train_ds, valid_ds = random_split(dataset, [n_train, n_valid])\n",
    "# train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class ATSTClassifier(nn.Module):\n",
    "#     def __init__(self, backbone, embed_dim=768, num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.backbone = backbone\n",
    "#         self.head = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, num_classes),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):  # x shape: [B, 1, T, F]\n",
    "#         embeddings = self.backbone(x)  # expected shape: [B, T, D]\n",
    "#         x_cls = embeddings[:, 0, :]    # pick token CLS\n",
    "#         return self.head(x_cls)\n",
    "\n",
    "\n",
    "\n",
    "# # %% ---------------------\n",
    "# from PretrainedSED.models.atstframe.ATSTF_wrapper import ATSTWrapper\n",
    "# from PretrainedSED.models.prediction_wrapper import PredictionsWrapper\n",
    "\n",
    "# # Crea backbone (ATST) e wrapper\n",
    "# atst = ATSTWrapper()\n",
    "# wrapper = PredictionsWrapper(atst, checkpoint=\"ATST-F_strong_1\", head_type=None)\n",
    "\n",
    "# # Copia i pesi dal wrapper nel tuo backbone\n",
    "# atst.load_state_dict(wrapper.model.state_dict(), strict=False)\n",
    "\n",
    "# # Ora usa il tuo modello con classificatore custom\n",
    "# model = ATSTClassifier(atst, embed_dim=768, num_classes=NUM_CLASSES)\n",
    "# # model.load_state_dict(torch.load(\"best_model.pt\", map_location=DEVICE))\n",
    "# model.to(DEVICE)\n",
    "# print('modello caricato con successo')\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\n",
    "\n",
    "# # %% ---------------------\n",
    "# def mean_average_precision_k(y_true, y_score, k):\n",
    "#     aps = []\n",
    "#     for true, score in zip(y_true, y_score):\n",
    "#         top_k = np.argsort(score)[::-1][:k]\n",
    "#         true_labels = np.where(true == 1)[0]\n",
    "#         hits, score_sum = 0, 0.0\n",
    "#         for i, label in enumerate(top_k):\n",
    "#             if label in true_labels:\n",
    "#                 hits += 1\n",
    "#                 score_sum += hits / (i + 1)\n",
    "#         aps.append(score_sum / min(len(true_labels), k) if len(true_labels) > 0 else 0.0)\n",
    "#     return np.mean(aps)\n",
    "\n",
    "# # %% ---------------------\n",
    "# best_loss = float('inf')\n",
    "# early_stop_counter = 0\n",
    "\n",
    "# train_losses, val_losses = [], []\n",
    "# f1_micros, map5s, map10s = [], [], []\n",
    "\n",
    "# print('Comincio il training')\n",
    "# for epoch in range(1, EPOCHS + 1):\n",
    "#     model.train()\n",
    "#     total_tr = 0.\n",
    "#     for X, y in tqdm(train_loader, desc=f\"Epoch {epoch:02d} [Train]\", leave=False):\n",
    "#         X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "#         logits = model(X)\n",
    "#         loss = criterion(logits, y)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_tr += loss.item() * X.size(0)\n",
    "#     train_loss = total_tr / n_train\n",
    "#     train_losses.append(train_loss)\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     total_val = 0.\n",
    "#     all_targets, all_preds, all_probs = [], [], []\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in tqdm(valid_loader, desc=f\"Epoch {epoch:02d} [Val]\", leave=False):\n",
    "#             X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "#             logits = model(X)\n",
    "#             loss = criterion(logits, y)\n",
    "#             total_val += loss.item() * X.size(0)\n",
    "#             probs = torch.sigmoid(logits).cpu().numpy()\n",
    "#             preds = (probs > 0.5).astype(int)\n",
    "#             all_targets.append(y.cpu().numpy())\n",
    "#             all_preds.append(preds)\n",
    "#             all_probs.append(probs)\n",
    "\n",
    "#     val_loss = total_val / n_valid\n",
    "#     val_losses.append(val_loss)\n",
    "\n",
    "#     y_true = np.vstack(all_targets)\n",
    "#     y_pred = np.vstack(all_preds)\n",
    "#     y_score = np.vstack(all_probs)\n",
    "\n",
    "#     f1_micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "#     map5 = mean_average_precision_k(y_true, y_score, 5)\n",
    "#     map10 = mean_average_precision_k(y_true, y_score, 10)\n",
    "\n",
    "#     f1_micros.append(f1_micro)\n",
    "#     map5s.append(map5)\n",
    "#     map10s.append(map10)\n",
    "\n",
    "#     print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f}, val_loss={val_loss:.4f} | F1_micro={f1_micro:.4f} | mAP@5={map5:.4f} | mAP@10={map10:.4f}\")\n",
    "\n",
    "#     if val_loss < best_loss:\n",
    "#         best_loss = val_loss\n",
    "#         torch.save(model.state_dict(), \"best_model.pt\")\n",
    "#         print(\"🏆 New best model saved\")\n",
    "#         early_stop_counter = 0\n",
    "#     else:\n",
    "#         early_stop_counter += 1\n",
    "#         if early_stop_counter >= PATIENCE:\n",
    "#             print(\"⏹ Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#     scheduler.step(val_loss)\n",
    "\n",
    "# # %% ---------------------\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.plot(train_losses, label='Train')\n",
    "# plt.plot(val_losses, label='Val')\n",
    "# plt.title(\"Loss\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.plot(f1_micros)\n",
    "# plt.title(\"F1-micro\")\n",
    "\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.plot(map5s, label=\"mAP@5\")\n",
    "# plt.plot(map10s, label=\"mAP@10\")\n",
    "# plt.title(\"mAP\")\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c292cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1daca7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dilation_list_t_dim: \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "MN(\n",
      "  (features): Sequential(\n",
      "    (0): ConvNormActivation(\n",
      "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=72, out_features=24, bias=True)\n",
      "              (fc2): Linear(in_features=24, out_features=72, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=120, out_features=32, bias=True)\n",
      "              (fc2): Linear(in_features=32, out_features=120, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(2, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=120, out_features=32, bias=True)\n",
      "              (fc2): Linear(in_features=32, out_features=120, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConvNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=480, out_features=120, bias=True)\n",
      "              (fc2): Linear(in_features=120, out_features=480, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=672, out_features=168, bias=True)\n",
      "              (fc2): Linear(in_features=168, out_features=672, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=672, out_features=168, bias=True)\n",
      "              (fc2): Linear(in_features=168, out_features=672, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=960, out_features=240, bias=True)\n",
      "              (fc2): Linear(in_features=240, out_features=960, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): ConvNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): ConvNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(2, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=960, out_features=240, bias=True)\n",
      "              (fc2): Linear(in_features=240, out_features=960, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ConvNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): ConvNormActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Loading pretrained checkpoint:  frame_mn10_strong_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c22ebc7bbeb44629cd187d7d34393f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep01[tr]:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95f78fdfccf4ec7a87379df0e660637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep01[val]:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep01 tr=1.572 val=1.376 F1=0.014 mAP5=0.014 mAP10=0.015\n",
      "   🔥 checkpoint!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc22cf9ca26f47059680bfc31d090cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep02[tr]:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58f06630cb34548a5fe117b878982c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep02[val]:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep02 tr=1.539 val=1.396 F1=0.013 mAP5=0.013 mAP10=0.014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7191671878f1408395b696ae609ce0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep03[tr]:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d6d56e92bc4bfc8e7cd4e618bd5c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep03[val]:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep03 tr=1.573 val=1.366 F1=0.017 mAP5=0.017 mAP10=0.018\n",
      "   🔥 checkpoint!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fb1b677c174ac5897a88646d7d1bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep04[tr]:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b5365e0a6642c09966f82bfc717437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep04[val]:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep04 tr=1.632 val=1.414 F1=0.014 mAP5=0.013 mAP10=0.015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8dc436b70841698d4462bdaf6fd904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep05[tr]:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276475dc6bfa48e389bc0e37df8ccb76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep05[val]:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep05 tr=1.642 val=1.455 F1=0.017 mAP5=0.016 mAP10=0.018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713492dfff6046f1b6ad79acadf9a408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep06[tr]:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1232caedd2a3414a88cf5990a5c7d181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep06[val]:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep06 tr=1.620 val=1.421 F1=0.016 mAP5=0.015 mAP10=0.016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9ddfdb46f74f31b79757aacc7935f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep07[tr]:   0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862f2ca38ab24c50a3a5159baccfbe67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep07[val]:   0%|          | 0/743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 163\u001b[39m\n\u001b[32m    161\u001b[39m net.eval(); val_loss = \u001b[32m0\u001b[39m; preds, targets = [], []\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEp\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mep\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m02d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m[val]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/torch/utils/data/dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mSegmentDS.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     d     = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     spec  = torch.tensor(d[\u001b[33m'\u001b[39m\u001b[33mlogmel\u001b[39m\u001b[33m'\u001b[39m]).float()   \u001b[38;5;66;03m# [T,F]\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.augment: spec = \u001b[38;5;28mself\u001b[39m._specaugment(spec)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:451\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    449\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# Fine‑tuning Frame‑MN (AudioSet‑strong ckpt) on MLPC segments\n",
    "# ==============================================================\n",
    "#  - 1st stage: freeze backbone, train head (5 epochs)\n",
    "#  - 2nd stage: fine‑tune whole net with lower LR\n",
    "#  - Regularisation: dropout head, weight‑decay, SpecAugment\n",
    "#  - Metrics: val‑loss, F1‑micro, mAP@5, mAP@10\n",
    "# ==============================================================\n",
    "\n",
    "##########################\n",
    "# 1) LIBRARIES & PATHS   #\n",
    "##########################\n",
    "import random, numpy as np, matplotlib.pyplot as plt, torch, torch.nn as nn, torch.optim as optim\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from PretrainedSED.models.frame_mn.Frame_MN_wrapper import FrameMNWrapper\n",
    "from PretrainedSED.models.prediction_wrapper import PredictionsWrapper\n",
    "\n",
    "SEGMENTS_DIR = Path(\"data/segments\")   # <-- cartella dei .npz\n",
    "DEVICE       = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MLPC_CLASSES = [\n",
    "    'Speech','Dog Bark','Rooster Crow','Shout',\n",
    "    'Lawn Mower','Chainsaw','Jackhammer',\n",
    "    'Power Drill','Horn Honk','Siren']\n",
    "NC = len(MLPC_CLASSES)\n",
    "\n",
    "##########################\n",
    "# 2) HYPER‑PARAMETERS    #\n",
    "##########################\n",
    "BATCH          = 32\n",
    "EPOCHS         = 40\n",
    "PATIENCE       = 5\n",
    "FREEZE_EPOCHS  = 5          # fase 1\n",
    "LR_HEAD_1      = 1e-3\n",
    "LR_HEAD_2      = 5e-4\n",
    "LR_BACKBONE    = 3e-5\n",
    "WD_HEAD        = 1e-4\n",
    "WD_BACKBONE    = 1e-2\n",
    "DROPOUT_P      = 0.5\n",
    "\n",
    "##########################\n",
    "# 3) DATASET + AUGMENT   #\n",
    "##########################\n",
    "class SegmentDS(torch.utils.data.Dataset):\n",
    "    \"\"\"Carica ogni segmento come log‑Mel (T,F) e label multi‑hot.\"\"\"\n",
    "    def __init__(self, folder: Path, augment: bool = False):\n",
    "        self.files   = list(folder.glob(\"*.npz\"))\n",
    "        self.augment = augment\n",
    "    def __len__(self): return len(self.files)\n",
    "    def _specaugment(self, spec: torch.Tensor):\n",
    "        \"\"\"Time‑mask & Freq‑mask soft.\"\"\"\n",
    "        # spec: [T,F]\n",
    "        if random.random() < 0.5:\n",
    "            t0 = random.randint(0, max(0, spec.shape[0]-10)); spec[t0:t0+10] = 0\n",
    "        if random.random() < 0.5:\n",
    "            f0 = random.randint(0, max(0, spec.shape[1]-4));  spec[:,f0:f0+4] = 0\n",
    "        return spec\n",
    "    def __getitem__(self, idx):\n",
    "        d     = np.load(self.files[idx])\n",
    "        spec  = torch.tensor(d['logmel']).float()   # [T,F]\n",
    "        if self.augment: spec = self._specaugment(spec)\n",
    "        x     = spec.unsqueeze(0)                   # [1,T,F]\n",
    "        y     = torch.tensor(d['labels']).float()\n",
    "        return x, y\n",
    "\n",
    "# split train / val\n",
    "full_ds        = SegmentDS(SEGMENTS_DIR)\n",
    "val_len        = int(0.2*len(full_ds))\n",
    "train_ds, val_ds = random_split(full_ds, [len(full_ds)-val_len, val_len], generator=torch.Generator().manual_seed(42))\n",
    "train_ds.dataset.augment = True  # solo train\n",
    "\n",
    "dl_tr  = DataLoader(train_ds, BATCH, shuffle=True)\n",
    "dl_val = DataLoader(val_ds,   BATCH)\n",
    "\n",
    "##########################\n",
    "# 4) CLASS‑IMBALANCE     #\n",
    "##########################\n",
    "#   pos_weight = (1‑p)/p  per BCE\n",
    "pos_cnt = torch.zeros(NC)\n",
    "for _, y in DataLoader(train_ds, BATCH):\n",
    "    pos_cnt += y.sum(0)\n",
    "pos_rate   = pos_cnt / len(train_ds)\n",
    "pos_weight = ((1.0 - pos_rate) / (pos_rate + 1e-6)).to(DEVICE)\n",
    "criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "##########################\n",
    "# 5) BACKBONE + HEAD     #\n",
    "##########################\n",
    "width   = 1.0                         # 0.6 → frame_mn06; 1.0 → frame_mn10\n",
    "backbone = FrameMNWrapper(width)\n",
    "embed_d  = backbone.state_dict()['frame_mn.features.16.1.bias'].shape[0]\n",
    "\n",
    "# carica ckpt AudioSet‑strong (+stripping head)\n",
    "wrapper = PredictionsWrapper(\n",
    "    backbone,\n",
    "    checkpoint=f\"frame_mn{int(width*10):02d}_strong_1\",\n",
    "    head_type=None)\n",
    "backbone.load_state_dict(wrapper.model.state_dict(), strict=False)\n",
    "\n",
    "# nuova testa\n",
    "class Head(nn.Sequential):\n",
    "    def __init__(self, in_d, n_cls, p=DROPOUT_P):\n",
    "        super().__init__(nn.Dropout(p), nn.Linear(in_d, n_cls))\n",
    "head = Head(embed_d, NC, DROPOUT_P)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, backbone, head):\n",
    "        super().__init__()\n",
    "        self.backbone, self.head = backbone, head\n",
    "    def forward(self, x):\n",
    "        z = self.backbone(x)      # [B,250,D]\n",
    "        return self.head(z[:,0])  # token CLS\n",
    "\n",
    "net = Net(backbone, head).to(DEVICE)\n",
    "\n",
    "##########################\n",
    "# 6) OPTIMIZER & SCHED   #\n",
    "##########################\n",
    "# fase 1: freeze backbone\n",
    "for p in net.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "optimizer = optim.AdamW(net.head.parameters(), lr=LR_HEAD_1, weight_decay=WD_HEAD)\n",
    "\n",
    "##########################\n",
    "# 7) mAP@k utility        #\n",
    "##########################\n",
    "def map_k(y_true, y_score, k=5):\n",
    "    \"\"\"Mean Average Precision @k (rows = samples).\"\"\"\n",
    "    aps = []\n",
    "    for t, s in zip(y_true, y_score):\n",
    "        topk = np.argsort(s)[::-1][:k]\n",
    "        rel  = set(np.where(t==1)[0])\n",
    "        hit = 0; score = 0.0\n",
    "        for i, lab in enumerate(topk, 1):\n",
    "            if lab in rel:\n",
    "                hit += 1; score += hit / i\n",
    "        aps.append(score / min(len(rel), k) if rel else 0.0)\n",
    "    return np.mean(aps)\n",
    "\n",
    "##########################\n",
    "# 8) TRAIN / VALID LOOP  #\n",
    "##########################\n",
    "H = {k:[] for k in ('tr','val','f1','m5','m10')}\n",
    "best_loss, bad = 1e9, 0\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    # ===== train =====\n",
    "    net.train(); tr_loss = 0\n",
    "    for x,y in tqdm(dl_tr, desc=f\"Ep{ep:02d}[tr]\", leave=False):\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(net(x), y)\n",
    "        loss.backward(); optimizer.step()\n",
    "        tr_loss += loss.item()*x.size(0)\n",
    "    tr_loss /= len(train_ds)\n",
    "\n",
    "    # ===== validation =====\n",
    "    net.eval(); val_loss = 0; preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for x,y in tqdm(dl_val, desc=f\"Ep{ep:02d}[val]\", leave=False):\n",
    "            x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "            logit = net(x)\n",
    "            val_loss += criterion(logit, y).item()*x.size(0)\n",
    "            preds.append(torch.sigmoid(logit).cpu()); targets.append(y.cpu())\n",
    "    val_loss /= len(val_ds)\n",
    "    P = torch.vstack(preds).numpy(); T = torch.vstack(targets).numpy()\n",
    "    f1  = f1_score(T, (P>.5).astype(int), average='micro', zero_division=0)\n",
    "    m5  = map_k(T,P,5); m10 = map_k(T,P,10)\n",
    "\n",
    "    # log\n",
    "    H['tr'].append(tr_loss); H['val'].append(val_loss)\n",
    "    H['f1'].append(f1); H['m5'].append(m5); H['m10'].append(m10)\n",
    "    print(f\"Ep{ep:02d} tr={tr_loss:.3f} val={val_loss:.3f} F1={f1:.3f} mAP5={m5:.3f} mAP10={m10:.3f}\")\n",
    "\n",
    "    # early‑stop & checkpoint\n",
    "    if val_loss < best_loss:\n",
    "        best_loss, bad = val_loss, 0\n",
    "        torch.save(net.state_dict(), \"best_frame_mn.pt\")\n",
    "        print(\"   🔥 checkpoint!\")\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad == PATIENCE:\n",
    "            print(\"   ⏹️  Early‑stop\"); break\n",
    "\n",
    "    # fine‑tune backbone dopo FREEZE_EPOCHS\n",
    "    if ep == FREEZE_EPOCHS:\n",
    "        for p in net.backbone.parameters():\n",
    "            p.requires_grad = True\n",
    "        optimizer = optim.AdamW([\n",
    "            {\"params\": net.head.parameters(),     \"lr\": LR_HEAD_2, \"weight_decay\": WD_HEAD},\n",
    "            {\"params\": net.backbone.parameters(), \"lr\": LR_BACKBONE, \"weight_decay\": WD_BACKBONE}\n",
    "        ])\n",
    "\n",
    "##########################\n",
    "# 9) PLOT                #\n",
    "##########################\n",
    "plt.figure(figsize=(13,4))\n",
    "plt.subplot(1,3,1); plt.plot(H['tr'],label='train'); plt.plot(H['val'],label='val'); plt.legend(); plt.title('Loss')\n",
    "plt.subplot(1,3,2); plt.plot(H['f1']); plt.title('F1‑micro')\n",
    "plt.subplot(1,3,3); plt.plot(H['m5'],label='mAP5'); plt.plot(H['m10'],label='mAP10'); plt.legend(); plt.title('mAP')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdee54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmwTeam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
