{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652bdf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target classes: ['Speech', 'Shout', 'Chainsaw', 'Jackhammer', 'Lawn Mower', 'Power Drill', 'Dog Bark', 'Rooster Crow', 'Horn Honk', 'Siren']\n",
      "Audio‑features : /Users/Q540900/Desktop/Sparkling---Pattern-Classification-Project/MLPC2025_test/audio_features\n",
      "Modelli        : /Users/Q540900/Desktop/Sparkling---Pattern-Classification-Project/04 - Model Training/models\n",
      "Caricati 10 modelli.\n",
      "Clip da processare: 0\n",
      "Caricate predizioni per 2742 file presenti nel ground-truth.\n",
      "Segment-level rows: 52191\n",
      "✓ predictions.csv creato e validato.\n",
      "Costo sistema: 22.28\n",
      "Costo baseline (all-zero): 0.00\n",
      "Δ miglioramento: -22.28\n",
      "Righe dopo merge: 52191\n",
      "Somma etichette ground-truth: 0\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # MLPC 2025 – Task 2 · Simple SED system  (v3 – autosufficiente con filtraggio e controllo)\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 1 · Setup generale\n",
    "import os, math, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "from compute_cost import (\n",
    "    CLASSES, COST_MATRIX, aggregate_targets,\n",
    "    total_cost, check_dataframe\n",
    ")\n",
    "\n",
    "DATASET_PATH  = Path(\"../MLPC2025_test\")\n",
    "AF_PATH       = DATASET_PATH / \"audio_features\"\n",
    "MODEL_DIR     = Path(\"./models\")\n",
    "FRAME_OUT_DIR = Path(\"./frame_out\")\n",
    "FRAME_OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "OUT_CSV       = \"predictions.csv\"\n",
    "\n",
    "print(\"Target classes:\", CLASSES)\n",
    "print(\"Audio‑features :\", AF_PATH.resolve())\n",
    "print(\"Modelli        :\", MODEL_DIR.resolve())\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 2 · Carica i modelli per classe\n",
    "models = {}\n",
    "for i, cls in enumerate(CLASSES):\n",
    "    pkl_path = MODEL_DIR / f\"classifier_{i}_{cls}.pkl\"\n",
    "    if not pkl_path.exists():\n",
    "        raise FileNotFoundError(f\"Modello mancante: {pkl_path}\")\n",
    "    models[cls] = joblib.load(pkl_path)\n",
    "print(f\"Caricati {len(models)} modelli.\")\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 3 · Genera (se manca) frame_out/<ID>.npz  con probs (T×10)\n",
    "def ensure_frame_predictions():\n",
    "    todo = []\n",
    "    for feat_file in AF_PATH.glob(\"*.npz\"):\n",
    "        file_id = feat_file.stem\n",
    "        if not (FRAME_OUT_DIR / f\"{file_id}.npz\").exists():\n",
    "            todo.append(file_id)\n",
    "\n",
    "    print(f\"Clip da processare: {len(todo)}\")\n",
    "\n",
    "    for fid in todo:\n",
    "        X = np.load(AF_PATH / f\"{fid}.npz\")[\"embeddings\"]  # T×D\n",
    "        probs = np.zeros((X.shape[0], len(CLASSES)), dtype=np.float32)\n",
    "        for j, cls in enumerate(CLASSES):\n",
    "            mdl = models[cls]\n",
    "            if hasattr(mdl, \"predict_proba\"):\n",
    "                probs[:, j] = mdl.predict_proba(X)[:, 1]\n",
    "            else:\n",
    "                z = mdl.decision_function(X)\n",
    "                probs[:, j] = 1 / (1 + np.exp(-z))\n",
    "        np.savez_compressed(FRAME_OUT_DIR / f\"{fid}.npz\", probs=probs)\n",
    "\n",
    "ensure_frame_predictions()\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 4 · Carica le predizioni frame-level filtrando sui file del ground-truth\n",
    "gt_df = pd.read_csv(DATASET_PATH / \"ground_truth.csv\")\n",
    "gt_df[\"onset\"] = np.round(gt_df[\"onset\"], 1)  # coerente con le predizioni\n",
    "valid_files = set(gt_df[\"filename\"].unique())\n",
    "\n",
    "def load_frame_level_predictions_filtered(pred_dir: Path, allowed_files: set) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "    mapping = {}\n",
    "    for npz_path in pred_dir.glob(\"*.npz\"):\n",
    "        fid = npz_path.stem\n",
    "        fname = f\"{fid}.mp3\"\n",
    "        if fname not in allowed_files:\n",
    "            continue\n",
    "        arr = np.load(npz_path)[\"probs\"]\n",
    "        assert arr.shape[1] == len(CLASSES)\n",
    "        mapping[fname] = {c: arr[:, i] for i, c in enumerate(CLASSES)}\n",
    "    print(f\"Caricate predizioni per {len(mapping)} file presenti nel ground-truth.\")\n",
    "    return mapping\n",
    "\n",
    "frame_preds = load_frame_level_predictions_filtered(FRAME_OUT_DIR, valid_files)\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 5 · Aggregazione 10 frame → 1.2s + thresholding 0.17 + filtro mediano\n",
    "def frame2segment(preds: Dict[str, Dict[str, np.ndarray]],\n",
    "                  thr: float = 0.17, use_median: bool = True) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for fname, per_class in preds.items():\n",
    "        frame_mat = np.stack([per_class[c] for c in CLASSES], axis=1)  # T×10\n",
    "        seg_mat   = aggregate_targets(frame_mat, f=10)                 # S×10\n",
    "        bin_mat   = (seg_mat > thr).astype(int)\n",
    "        if use_median and bin_mat.shape[0] >= 3:\n",
    "            bin_mat = medfilt(bin_mat, kernel_size=(3, 1))\n",
    "        for idx, vec in enumerate(bin_mat):\n",
    "            onset = np.round(idx * 1.2, 1)\n",
    "            rows.append([fname, onset] + vec.tolist())\n",
    "    return pd.DataFrame(rows, columns=[\"filename\", \"onset\"] + CLASSES)\n",
    "\n",
    "seg_df = frame2segment(frame_preds)\n",
    "print(\"Segment-level rows:\", len(seg_df))\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 6 · Salvataggio + validazione formato\n",
    "seg_df.to_csv(OUT_CSV, index=False)\n",
    "check_dataframe(seg_df, dataset_path=str(DATASET_PATH))\n",
    "print(f\"✓ {OUT_CSV} creato e validato.\")\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 7 · Valutazione costo (vs baseline 0)\n",
    "cost_sys, _ = total_cost(seg_df, gt_df)\n",
    "print(f\"Costo sistema: {cost_sys:.2f}\")\n",
    "\n",
    "zero_pred = gt_df.copy()\n",
    "zero_pred[CLASSES] = 0\n",
    "cost_zero, _ = total_cost(zero_pred, gt_df)\n",
    "print(f\"Costo baseline (all-zero): {cost_zero:.2f}\")\n",
    "print(f\"Δ miglioramento: {cost_zero - cost_sys:.2f}\")\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 8 · Controllo allineamento predizioni e GT\n",
    "merged = seg_df.merge(gt_df, on=[\"filename\", \"onset\"], suffixes=(\"_p\", \"_t\"))\n",
    "print(\"Righe dopo merge:\", len(merged))\n",
    "print(\"Somma etichette ground-truth:\", merged[[c + \"_t\" for c in CLASSES]].values.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195eac30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmwTeam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
