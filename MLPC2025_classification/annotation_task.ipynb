{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea58d87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percorsi impostati ✅\n",
      "Ground‑truth validi: 27551\n",
      "Numero classi: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valuta: 100%|██████████| 8230/8230 [00:32<00:00, 252.87file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.csv salvato → eval_out\n",
      "Macro-F1  : 0.0291\n",
      "Micro-F1  : 0.0269\n",
      "Micro-P   : 0.0337\n",
      "Micro-R   : 0.0224\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Notebook  :  LF evaluation – exercise (a)  (✱ versione corretta ✱)\n",
    "# Obiettivo  : misurare precision/recall/F1 delle labeling‑functions (LF)\n",
    "#              e verificare l’udibilità degli eventi.\n",
    "# Peculiarità dataset\n",
    "#   • `labels/{idx}_labels.npz`   → una chiave per classe, valore = (N,2) segmenti.\n",
    "#   • `audio_features/{idx}.npz`  → contiene \"melspectrogram\" (T,F).\n",
    "#   • `annotations.csv`           → colonne: filename,onset,offset,categories (lista).\n",
    "#   • `metadata.csv` (opz.)       → start_time_s, end_time_s: ritaglio del clip.\n",
    "# Output\n",
    "#   eval_out/results.csv          → metriche per classe + breakdown audible/silent\n",
    "#   eval_out/overall_metrics.txt  → macro/micro P‑R‑F1\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 0. Dipendenze\n",
    "# ```bash\n",
    "# # esegui solo se mancano\n",
    "# !pip install pandas numpy intervaltree scipy tqdm\n",
    "# ```\n",
    "\n",
    "# %%\n",
    "# 1. Path (hard‑coded)\n",
    "from pathlib import Path\n",
    "\n",
    "ANN_PATH  = Path(\"annotations.csv\")     # ground‑truth\n",
    "META_PATH = Path(\"metadata.csv\")        # facoltativo\n",
    "LABEL_DIR = Path(\"labels\")              # label npz\n",
    "FEAT_DIR  = Path(\"audio_features\")      # feature npz\n",
    "OUT_DIR   = Path(\"eval_out\")\n",
    "\n",
    "IOU_THRESH       = 0.3      # soglia più tollerante (prima era 0.5)\n",
    "ENERGY_THRESH_DB = -50.0    # udibile se > soglia dB\n",
    "MERGE_GAP        = 0.15     # unisci segmenti LF con gap < 150 ms\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Percorsi impostati ✅\")\n",
    "\n",
    "# %%\n",
    "# 2. Import\n",
    "import ast, math, json, itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from intervaltree import Interval, IntervalTree\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# %%\n",
    "# 3. Helper\n",
    "\n",
    "def iou(a, b):\n",
    "    inter = max(0, min(a[1], b[1]) - max(a[0], b[0]))\n",
    "    if inter == 0:\n",
    "        return 0.0\n",
    "    union = max(a[1], b[1]) - min(a[0], b[0])\n",
    "    return inter / union\n",
    "\n",
    "\n",
    "def seg_mean_db(logmel, sr, hop, seg):\n",
    "    t0, t1 = seg\n",
    "    f0 = int(t0 * sr / hop)\n",
    "    f1 = int(math.ceil(t1 * sr / hop))\n",
    "    f1 = min(f1, logmel.shape[0])\n",
    "    return float(logmel[f0:f1].mean()) if f1 > f0 else -np.inf\n",
    "\n",
    "\n",
    "def load_pred_segments(path: Path):\n",
    "    \"\"\"Return merged (segments, class) lists after lower/strip & small‑gap merge.\"\"\"\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    raw = []\n",
    "    for cls in data.files:\n",
    "        segs = data[cls]\n",
    "        if segs.ndim != 2 or segs.shape[1] != 2:\n",
    "            continue\n",
    "        cls_norm = cls.strip().lower()\n",
    "        for s in segs.astype(float):\n",
    "            raw.append((tuple(s), cls_norm))\n",
    "\n",
    "    # sort by start time\n",
    "    raw.sort(key=lambda x: (x[1], x[0][0]))\n",
    "\n",
    "    merged = []\n",
    "    for cls, grp in itertools.groupby(raw, key=lambda x: x[1]):\n",
    "        grp = [s for s, _ in grp]\n",
    "        grp.sort()\n",
    "        cur = list(grp[0])\n",
    "        for seg in grp[1:]:\n",
    "            if seg[0] - cur[1] <= MERGE_GAP:   # overlap / tiny gap ⇒ merge\n",
    "                cur[1] = max(cur[1], seg[1])\n",
    "            else:\n",
    "                merged.append((tuple(cur), cls))\n",
    "                cur = list(seg)\n",
    "        merged.append((tuple(cur), cls))\n",
    "    segs, cats = zip(*merged) if merged else ([], [])\n",
    "    return list(segs), list(cats)\n",
    "\n",
    "# %%\n",
    "# 4. Carica annotazioni + metadata e normalizza\n",
    "ann = pd.read_csv(ANN_PATH)\n",
    "ann[\"categories\"] = ann[\"categories\"].apply(ast.literal_eval)\n",
    "\n",
    "# normalizza stringhe\n",
    "ann[\"categories\"] = ann[\"categories\"].apply(lambda lst: [c.lower().strip() for c in lst])\n",
    "ann[\"filename\"]   = ann[\"filename\"].str.strip()\n",
    "\n",
    "# merge con metadata se esiste → sottrai start_time_s per avere tempi relativi al clip\n",
    "if META_PATH.exists():\n",
    "    meta = pd.read_csv(META_PATH, usecols=[\"filename\", \"start_time_s\", \"end_time_s\"])\n",
    "    ann = ann.merge(meta, on=\"filename\", how=\"left\")\n",
    "    ann[\"onset\"]  = ann[\"onset\"]  - ann[\"start_time_s\"].fillna(0)\n",
    "    ann[\"offset\"] = ann[\"offset\"] - ann[\"start_time_s\"].fillna(0)\n",
    "\n",
    "# indice del clip\n",
    "ann[\"idx\"] = ann[\"filename\"].str.replace(\".mp3\", \"\", regex=False)\n",
    "\n",
    "# filtra intervalli nulli / negativi\n",
    "ann = ann[ann[\"onset\"] < ann[\"offset\"]]\n",
    "print(f\"Ground‑truth validi: {len(ann)}\")\n",
    "\n",
    "CLASSES = sorted({c for cats in ann[\"categories\"] for c in cats})\n",
    "print(\"Numero classi:\", len(CLASSES))\n",
    "\n",
    "# %%\n",
    "# 5. Funzione di valutazione (con clipping sui limiti del frammento)\n",
    "\n",
    "def evaluate(ann_df, lbl_dir, feat_dir):\n",
    "    stats = {c: dict(TP=0, FP=0, FN=0, TP_audible=0, FP_silent=0) for c in CLASSES}\n",
    "\n",
    "    for idx, gt_rows_all in tqdm(ann_df.groupby(\"idx\"), desc=\"valuta\", unit=\"file\"):\n",
    "        pred_path = lbl_dir / f\"{idx}_labels.npz\"\n",
    "        if not pred_path.exists():\n",
    "            continue\n",
    "        segs_pred, cats_pred = load_pred_segments(pred_path)\n",
    "\n",
    "        # feature per durata + audibilità\n",
    "        feat_path = feat_dir / f\"{idx}.npz\"\n",
    "        have_feat = feat_path.exists()\n",
    "        if have_feat:\n",
    "            feat = np.load(feat_path, allow_pickle=True)\n",
    "            hop = int(feat.get(\"hop_length\", 512))\n",
    "            sr  = int(feat.get(\"sample_rate\", 32000))\n",
    "            n_frames = feat[\"melspectrogram\"].shape[0]\n",
    "            clip_dur = n_frames * hop / sr\n",
    "            logmel = 10*np.log10(np.maximum(feat[\"melspectrogram\"].astype(float),1e-12))\n",
    "        else:\n",
    "            # se mancano le feature, stima durata dal max(frame)\n",
    "            clip_dur = max((s[1] for s in segs_pred), default=0)\n",
    "\n",
    "        # ---- filtro GT: tieni solo intervalli che intersecano [0, clip_dur]\n",
    "        gt_rows = gt_rows_all[(gt_rows_all.offset > 0) & (gt_rows_all.onset < clip_dur)]\n",
    "        if gt_rows.empty and not segs_pred:\n",
    "            continue  # nulla da valutare\n",
    "\n",
    "        # IntervalTree GT by class\n",
    "        tree = defaultdict(IntervalTree)\n",
    "        for r in gt_rows.itertuples():\n",
    "            # clamp nei limiti del frammento\n",
    "            a = max(0.0, r.onset)\n",
    "            b = min(clip_dur, r.offset)\n",
    "            for cat in r.categories:\n",
    "                tree[cat].add(Interval(a, b))\n",
    "\n",
    "        matched = {c: set() for c in CLASSES}\n",
    "\n",
    "        for seg, cat in zip(segs_pred, cats_pred):\n",
    "            if cat not in stats:\n",
    "                continue\n",
    "            t0, t1 = seg\n",
    "            if t0 >= clip_dur or t1 <= 0:\n",
    "                continue  # predizione fuori frammento\n",
    "\n",
    "            overlaps = [iv for iv in tree[cat].overlap(t0, t1) if iou(seg, (iv.begin, iv.end)) >= IOU_THRESH]\n",
    "            tp = bool(overlaps)\n",
    "\n",
    "            audible = False\n",
    "            if have_feat:\n",
    "                audible = seg_mean_db(logmel, sr, hop, seg) > ENERGY_THRESH_DB\n",
    "\n",
    "            if tp:\n",
    "                stats[cat][\"TP\"] += 1\n",
    "                if audible:\n",
    "                    stats[cat][\"TP_audible\"] += 1\n",
    "                matched[cat].add((overlaps[0].begin, overlaps[0].end))\n",
    "            else:\n",
    "                stats[cat][\"FP\"] += 1\n",
    "                if not audible:\n",
    "                    stats[cat][\"FP_silent\"] += 1\n",
    "\n",
    "        # conteggia FN\n",
    "        for cat, tr in tree.items():\n",
    "            stats[cat][\"FN\"] += len(tr) - len(matched[cat])\n",
    "\n",
    "    # build DataFrame\n",
    "    rows = []\n",
    "    for cat, s in stats.items():\n",
    "        tp, fp, fn = s[\"TP\"], s[\"FP\"], s[\"FN\"]\n",
    "        prec = tp / (tp + fp + 1e-9)\n",
    "        rec  = tp / (tp + fn + 1e-9)\n",
    "        f1   = 2*prec*rec/(prec+rec+1e-9)\n",
    "        rows.append({\"class\":cat, **s, \"precision\":prec, \"recall\":rec, \"f1\":f1})\n",
    "    return pd.DataFrame(rows).sort_values(\"class\")\n",
    "\n",
    "# 6. Esegui valutazione Esegui valutazione\n",
    "\n",
    "df = evaluate(ann, LABEL_DIR, FEAT_DIR)\n",
    "\n",
    "df.to_csv(OUT_DIR/\"results.csv\", index=False)\n",
    "print(\"results.csv salvato →\", OUT_DIR)\n",
    "df.head()\n",
    "\n",
    "# %%\n",
    "# 7. Macro/Micro\n",
    "macro_f1 = df[\"f1\"].mean()\n",
    "micro_tp = df[\"TP\"].sum(); micro_fp = df[\"FP\"].sum(); micro_fn = df[\"FN\"].sum()\n",
    "micro_p  = micro_tp/(micro_tp+micro_fp+1e-9)\n",
    "micro_r  = micro_tp/(micro_tp+micro_fn+1e-9)\n",
    "micro_f1 = 2*micro_p*micro_r/(micro_p+micro_r+1e-9)\n",
    "\n",
    "with open(OUT_DIR/\"overall_metrics.txt\",\"w\") as f:\n",
    "    f.write(f\"Macro-F1  : {macro_f1:.4f}\\n\"\n",
    "            f\"Micro-F1  : {micro_f1:.4f}\\n\"\n",
    "            f\"Micro-P   : {micro_p:.4f}\\n\"\n",
    "            f\"Micro-R   : {micro_r:.4f}\\n\")\n",
    "print((OUT_DIR/\"overall_metrics.txt\").read_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d59552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classi con FP ma mai TP: 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP_audible</th>\n",
       "      <th>FP_silent</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beep/bleep</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>saxophone</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>sneeze</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ship/boat</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cough</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lawn mower</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cowbell</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rooster crow</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>trumpet</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>doorbell</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>horse neigh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>pig oink</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class  TP  FP   FN  TP_audible  FP_silent  precision  recall   f1\n",
       "2     beep/bleep   0  44  141           0         22        0.0     0.0  0.0\n",
       "38     saxophone   0  14   65           0          6        0.0     0.0  0.0\n",
       "45        sneeze   0  12    9           0         10        0.0     0.0  0.0\n",
       "41     ship/boat   0  10   81           0          0        0.0     0.0  0.0\n",
       "11         cough   0   9   38           0          1        0.0     0.0  0.0\n",
       "30    lawn mower   0   8   52           0          2        0.0     0.0  0.0\n",
       "13       cowbell   0   6   32           0          0        0.0     0.0  0.0\n",
       "37  rooster crow   0   6   22           0          0        0.0     0.0  0.0\n",
       "52       trumpet   0   5   39           0          0        0.0     0.0  0.0\n",
       "16      doorbell   0   4   17           0          0        0.0     0.0  0.0\n",
       "26   horse neigh   0   2   17           0          0        0.0     0.0  0.0\n",
       "33      pig oink   0   2   27           0          0        0.0     0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'labels/labels/222809_labels.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# %% ➌ –  distribuzione lunghezze predette vs GT\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m sns.histplot([iv[\u001b[32m1\u001b[39m]-iv[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m iv \u001b[38;5;129;01min\u001b[39;00m itertools.chain(*[\u001b[43mload_pred_segments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLABEL_DIR\u001b[49m\u001b[43m/\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m] \n\u001b[32m     15\u001b[39m                                                       \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m LABEL_DIR.iterdir()])],\n\u001b[32m     16\u001b[39m              binwidth=\u001b[32m0.1\u001b[39m, stat=\u001b[33m\"\u001b[39m\u001b[33mprobability\u001b[39m\u001b[33m\"\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mLF\u001b[39m\u001b[33m\"\u001b[39m, color=\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m sns.histplot((ann[\u001b[33m\"\u001b[39m\u001b[33moffset\u001b[39m\u001b[33m\"\u001b[39m]-ann[\u001b[33m\"\u001b[39m\u001b[33monset\u001b[39m\u001b[33m\"\u001b[39m]).clip(lower=\u001b[32m0\u001b[39m), binwidth=\u001b[32m0.1\u001b[39m,\n\u001b[32m     18\u001b[39m              stat=\u001b[33m\"\u001b[39m\u001b[33mprobability\u001b[39m\u001b[33m\"\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mGT\u001b[39m\u001b[33m\"\u001b[39m, color=\u001b[33m\"\u001b[39m\u001b[33mg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m plt.xlim(\u001b[32m0\u001b[39m, \u001b[32m5\u001b[39m); plt.legend(); plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33msegment length (s)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mload_pred_segments\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_pred_segments\u001b[39m(path: Path):\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return merged (segments, class) lists after lower/strip & small‑gap merge.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     raw = []\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data.files:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/micromamba/2.0.8/envs/bmwTeam/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:451\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    449\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'labels/labels/222809_labels.npz'"
     ]
    }
   ],
   "source": [
    "# %% ➊ – panoramica per classe\n",
    "df = pd.read_csv(OUT_DIR / \"results.csv\")\n",
    "(df.sort_values(\"f1\")\n",
    "   [[\"class\", \"TP\", \"FP\", \"FN\", \"precision\", \"recall\", \"f1\"]]\n",
    "   .head(15))\n",
    "\n",
    "# %% ➋ –  scova le classi con tanti FP e zero TP\n",
    "bad = df[(df[\"TP\"] == 0) & (df[\"FP\"] > 0)]\n",
    "print(\"# classi con FP ma mai TP:\", len(bad))\n",
    "display(bad.sort_values(\"FP\", ascending=False).head(20))\n",
    "\n",
    "# %% ➌ –  distribuzione lunghezze predette vs GT\n",
    "import seaborn as sns\n",
    "sns.histplot([iv[1]-iv[0] for iv in itertools.chain(*[load_pred_segments(LABEL_DIR/p)[0] \n",
    "                                                      for p in LABEL_DIR.iterdir()])],\n",
    "             binwidth=0.1, stat=\"probability\", label=\"LF\", color=\"r\")\n",
    "sns.histplot((ann[\"offset\"]-ann[\"onset\"]).clip(lower=0), binwidth=0.1,\n",
    "             stat=\"probability\", label=\"GT\", color=\"g\")\n",
    "plt.xlim(0, 5); plt.legend(); plt.xlabel(\"segment length (s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "937a24b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗  Classi che LF usa ma GT non conosce: []\n",
      "✗  Classi che GT ha ma LF non produce: []\n",
      "✗  Classi che non raggiungono mai 50 % di copertura: ['crying', 'waves', 'cowbell', 'wind', 'cow moo', 'truck', 'car', 'bus', 'washing machine', 'doorbell', 'lawn mower', 'motorcycle']\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(iou((s0,s1),(r.onset,r.offset))>=\u001b[32m0.3\u001b[39m \n\u001b[32m     40\u001b[39m                    \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ann[(ann.idx==idx)].itertuples() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m r.categories):\n\u001b[32m     41\u001b[39m             fp_idx, fp_seg = idx, (s0,s1)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCand-FP  →\u001b[39m\u001b[33m\"\u001b[39m, fp_idx, fp_seg)\n",
      "\u001b[31mStopIteration\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# LF: classi che appaiono almeno una volta tra le chiavi .npz\n",
    "lf_vocab = set()\n",
    "for p in LABEL_DIR.iterdir():\n",
    "    lf_vocab.update([cls.strip().lower() for cls in np.load(p, allow_pickle=True).files])\n",
    "\n",
    "# GT: classi usate nelle annotazioni\n",
    "gt_vocab = {c for cats in ann[\"categories\"] for c in cats}\n",
    "\n",
    "extra = sorted(lf_vocab - gt_vocab)\n",
    "missing = sorted(gt_vocab - lf_vocab)\n",
    "\n",
    "print(\"✗  Classi che LF usa ma GT non conosce:\", extra[:15])\n",
    "print(\"✗  Classi che GT ha ma LF non produce:\", missing[:15])\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "gt_cover = Counter()   # percentuale GT coperta da almeno una LF\n",
    "\n",
    "for idx, grp in ann.groupby(\"idx\"):\n",
    "    segs, cats = load_pred_segments(LABEL_DIR / f\"{idx}_labels.npz\")\n",
    "    for r in grp.itertuples():\n",
    "        for (s0,s1), c in zip(segs, cats):\n",
    "            if c in r.categories:\n",
    "                inter = max(0, min(s1, r.offset) - max(s0, r.onset))\n",
    "                ratio  = inter / (r.offset - r.onset)\n",
    "                gt_cover[c] += ratio >= 0.5\n",
    "                break\n",
    "\n",
    "bad_cover = [k for k,v in gt_cover.items() if v == 0]\n",
    "print(\"✗  Classi che non raggiungono mai 50 % di copertura:\", bad_cover[:12])\n",
    "\n",
    "\n",
    "# trova un FP di 'doorbell'\n",
    "cls = \"doorbell\"\n",
    "for idx in ann[\"idx\"].unique():\n",
    "    segs, cats = load_pred_segments(LABEL_DIR / f\"{idx}_labels.npz\")\n",
    "    for (s0,s1), c in zip(segs, cats):\n",
    "        if c != cls: continue\n",
    "        if not any(iou((s0,s1),(r.onset,r.offset))>=0.3 \n",
    "                   for r in ann[(ann.idx==idx)].itertuples() if cls in r.categories):\n",
    "            fp_idx, fp_seg = idx, (s0,s1)\n",
    "            raise StopIteration\n",
    "print(\"Cand-FP  →\", fp_idx, fp_seg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1af711bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percorsi impostati ✅\n",
      "GT validi: 27551, clip: 8230, classi: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|██████████| 8230/8230 [00:30<00:00, 271.84file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Salvato results.csv in eval_out\n",
      "Macro-F1  : 0.4509\n",
      "Micro-F1  : 0.4425\n",
      "Micro-P   : 0.4663\n",
      "Micro-R   : 0.4210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ╔════════════════════════════════════════════════════════════╗\n",
    "# ║          LF evaluation – esercizio (a)  (REWRITE)         ║\n",
    "# ║               * Ground‑truth vs Labeling‑Functions *       ║\n",
    "# ╚════════════════════════════════════════════════════════════╝\n",
    "# Questo notebook, da eseguire cell‑per‑cell, parte **da zero**:\n",
    "#   1. Carica `annotations.csv` (GT) e `metadata.csv` (start/end clip)\n",
    "#   2. Converte le coordinate GT in timeline «clip‑relativa»\n",
    "#   3. Legge le prediction LF (una chiave per classe, valori = segmenti)\n",
    "#      * se i segmenti sono in frame → li converte in secondi\n",
    "#      * li trasla anch’essi di `-start_time_s` se necessario\n",
    "#   4. Valuta TP / FP / FN con overlap ≥ 30 % del GT\n",
    "#   5. Calcola precision/recall/F1 + breakdown udibile/silenzioso\n",
    "#   6. Salva `results.csv` e `overall_metrics.txt` in `eval_out/`\n",
    "# --------------------------------------------------------------------\n",
    "# Dipendenze (esegui una volta):\n",
    "# !pip install pandas numpy intervaltree scipy tqdm\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# %% 0 – CONFIG                       ★★★ EDITA SOLO QUI ★★★\n",
    "from pathlib import Path\n",
    "\n",
    "ANN_PATH   = Path(\"annotations.csv\")     # ground‑truth\n",
    "META_PATH  = Path(\"metadata.csv\")        # start_time_s / end_time_s\n",
    "LABEL_DIR  = Path(\"labels\")              # *.npz con prediction LF\n",
    "FEAT_DIR   = Path(\"audio_features\")      # *.npz con melspectrogram\n",
    "OUT_DIR    = Path(\"eval_out\")            # dove salvare i risultati\n",
    "\n",
    "SR_DEFAULT   = 32_000     # sample‑rate se mancante\n",
    "HOP_DEFAULT  = 512        # hop_length se mancante\n",
    "IOU_GT_RATIO = 0.3        # minimo 30 % del GT coperto → TP\n",
    "MERGE_GAP    = 0.30       # unisci segmenti LF con gap < 300 ms\n",
    "ENERGY_DB_TH = -50.0      # soglia dB per dire «udibile»\n",
    "\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "print(\"Percorsi impostati ✅\")\n",
    "\n",
    "# %% 1 – IMPORT STANDARD\n",
    "import ast, json, math, itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from intervaltree import Interval, IntervalTree\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %% 2 – CARICA ANNOTAZIONI + METADATA, NORMALIZZA\n",
    "ann = pd.read_csv(ANN_PATH)\n",
    "ann[\"categories\"] = ann[\"categories\"].apply(ast.literal_eval)\n",
    "ann[\"categories\"] = ann[\"categories\"].apply(lambda L: [c.lower().strip() for c in L])\n",
    "\n",
    "if META_PATH.exists():\n",
    "    meta = pd.read_csv(META_PATH, usecols=[\"filename\", \"start_time_s\", \"end_time_s\"])\n",
    "    ann = ann.merge(meta, on=\"filename\", how=\"left\")\n",
    "    ann[\"onset\"]  = ann[\"onset\"]  - ann[\"start_time_s\"].fillna(0)\n",
    "    ann[\"offset\"] = ann[\"offset\"] - ann[\"start_time_s\"].fillna(0)\n",
    "else:\n",
    "    ann[\"start_time_s\"] = 0.0\n",
    "\n",
    "# rimuovi intervalli nulli / negativi\n",
    "ann = ann[ann[\"onset\"] < ann[\"offset\"]]\n",
    "ann[\"idx\"] = ann[\"filename\"].str.replace(\".mp3\", \"\", regex=False)\n",
    "\n",
    "CLASSES = sorted({c for cats in ann[\"categories\"] for c in cats})\n",
    "print(f\"GT validi: {len(ann)}, clip: {ann['idx'].nunique()}, classi: {len(CLASSES)}\")\n",
    "\n",
    "# %% 3 – HELPER FUNZIONI\n",
    "\n",
    "def covers_gt(pred, gt, thr=IOU_GT_RATIO):\n",
    "    inter = max(0, min(pred[1], gt[1]) - max(pred[0], gt[0]))\n",
    "    return inter / (gt[1]-gt[0]) >= thr\n",
    "\n",
    "\n",
    "def merge_segments(segs, gap=MERGE_GAP):\n",
    "    \"\"\"segs = list[(start,end)] già ordinati; merge se gap<gap\"\"\"\n",
    "    if not segs:\n",
    "        return []\n",
    "    merged = [list(segs[0])]\n",
    "    for a, b in segs[1:]:\n",
    "        if a - merged[-1][1] <= gap:\n",
    "            merged[-1][1] = max(merged[-1][1], b)\n",
    "        else:\n",
    "            merged.append([a, b])\n",
    "    return [tuple(s) for s in merged]\n",
    "\n",
    "\n",
    "def frames_to_segments(mask: np.ndarray, sr: int, hop: int):\n",
    "    \"\"\"Convert a 1‑D binary/probability mask (per frame) into (start,end) segments in **seconds**.\"\"\"\n",
    "    onsets = np.where(mask[:-1] <= 0)[0] + 1  # start after zero→nonzero\n",
    "    onsets = np.insert(onsets, 0, 0) if mask[0] > 0 else onsets\n",
    "    offsets = np.where(mask[1:] <= 0)[0] + 1  # first zero after nonzero\n",
    "    offsets = np.append(offsets, len(mask)) if mask[-1] > 0 else offsets\n",
    "    return [(o * hop / sr, off * hop / sr) for o, off in zip(onsets, offsets) if off > o]\n",
    "\n",
    "\n",
    "def load_pred_segments(path, clip_len, start_shift=0.0, sr=SR_DEFAULT, hop=HOP_DEFAULT):\n",
    "    \"\"\"\n",
    "    Restituisce (segments, class) già:\n",
    "      • convertiti in secondi (se erano frame)\n",
    "      • traslati di -start_shift (clip‑relativi)\n",
    "      • filtrati fuori dal range [0, clip_len]\n",
    "      • uniti con gap < MERGE_GAP\n",
    "    \"\"\"\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    segs_by_cat = defaultdict(list)\n",
    "\n",
    "    for cls in data.files:\n",
    "        arr = np.asarray(data[cls], dtype=float)\n",
    "        cls_norm = cls.strip().lower()\n",
    "\n",
    "                # ───────── decodifica formato ─────────\n",
    "        if arr.ndim == 2 and arr.shape[1] in {2,3}:   # (N,2) o (N,3)\n",
    "            if arr.shape[1] == 3:                     # probabilità per frame\n",
    "                mask = arr[:, 0]                      # usa primo canale p(event)\n",
    "                if mask.max() <= 0:\n",
    "                    continue\n",
    "                cand = frames_to_segments(mask, sr, hop)\n",
    "            else:                                     # (N,2)\n",
    "                is_frame = arr.max() > clip_len * 10\n",
    "                cand = [(tuple(s * hop / sr) if is_frame else tuple(s)) for s in arr]\n",
    "\n",
    "            for seg in cand:\n",
    "                seg = (seg[0] - start_shift, seg[1] - start_shift)\n",
    "                if seg[1] <= 0 or seg[0] >= clip_len:\n",
    "                    continue\n",
    "                segs_by_cat[cls_norm].append(seg)\n",
    "\n",
    "        elif arr.ndim in {1, 2} and arr.shape[-1] == 1:  # mask per frame (N,1) o (N,)\n",
    "            mask = arr.squeeze()\n",
    "            if mask.max() <= 0:\n",
    "                continue\n",
    "            for seg in frames_to_segments(mask, sr, hop):\n",
    "                seg = (seg[0] - start_shift, seg[1] - start_shift)\n",
    "                if seg[1] <= 0 or seg[0] >= clip_len:\n",
    "                    continue\n",
    "                segs_by_cat[cls_norm].append(seg)\n",
    "        else:\n",
    "            print(\"⚠️ forma non gestita:\", cls, arr.shape)\n",
    "            print(\"⚠️ forma non gestita:\", cls, arr.shape)\n",
    "\n",
    "    # merge + restituisci\n",
    "    segs_final, cats_final = [], []\n",
    "    for cat, lst in segs_by_cat.items():\n",
    "        lst.sort()\n",
    "        for m in merge_segments(lst):\n",
    "            segs_final.append(m); cats_final.append(cat)\n",
    "    return segs_final, cats_final\n",
    "\n",
    "# %% 4 – LOOP DI VALUTAZIONE\n",
    "stats = {c: dict(TP=0, FP=0, FN=0) for c in CLASSES}\n",
    "\n",
    "groups = ann.groupby(\"idx\")\n",
    "for idx, gt_rows in tqdm(groups, desc=\"eval\", unit=\"file\"):\n",
    "    start_sec = float(gt_rows[\"start_time_s\"].iloc[0])\n",
    "\n",
    "    # durata clip in secondi dal feature file\n",
    "    feat_path = FEAT_DIR / f\"{idx}.npz\"\n",
    "    if not feat_path.exists():\n",
    "        continue  # manca feature → salta clip\n",
    "    feat = np.load(feat_path, allow_pickle=True)\n",
    "    hop = int(feat.get(\"hop_length\", HOP_DEFAULT))\n",
    "    sr  = int(feat.get(\"sample_rate\",  SR_DEFAULT))\n",
    "    n_frames = feat[\"melspectrogram\"].shape[0]\n",
    "    clip_len = n_frames * hop / sr\n",
    "\n",
    "    segs_pred, cats_pred = load_pred_segments(\n",
    "        LABEL_DIR / f\"{idx}_labels.npz\",\n",
    "        clip_len=clip_len,\n",
    "        start_shift=start_sec,\n",
    "        sr=sr, hop=hop,\n",
    "    )\n",
    "\n",
    "    # costruisci IntervalTree GT per classe\n",
    "    tree = defaultdict(IntervalTree)\n",
    "    for r in gt_rows.itertuples():\n",
    "        for cat in r.categories:\n",
    "            a, b = max(0, r.onset), max(0, r.offset)\n",
    "            if b <= a or a >= clip_len:\n",
    "                continue\n",
    "            tree[cat].add(Interval(a, min(b, clip_len)))\n",
    "\n",
    "    matched = {c: set() for c in CLASSES}\n",
    "\n",
    "    for seg, cat in zip(segs_pred, cats_pred):\n",
    "        if cat not in stats:\n",
    "            continue\n",
    "        overlaps = [iv for iv in tree[cat].overlap(*seg) if covers_gt(seg, (iv.begin, iv.end))]\n",
    "        if overlaps:\n",
    "            stats[cat][\"TP\"] += 1\n",
    "            matched[cat].add((overlaps[0].begin, overlaps[0].end))\n",
    "        else:\n",
    "            stats[cat][\"FP\"] += 1\n",
    "\n",
    "    for cat, tr in tree.items():\n",
    "        stats[cat][\"FN\"] += len(tr) - len(matched[cat])\n",
    "\n",
    "# %% 5 – BUILD METRICS DF – BUILD METRICS DF\n",
    "rows = []\n",
    "for cat, s in stats.items():\n",
    "    tp, fp, fn = s.values()\n",
    "    prec = tp / (tp+fp+1e-9)\n",
    "    rec  = tp / (tp+fn+1e-9)\n",
    "    f1   = 2*prec*rec/(prec+rec+1e-9)\n",
    "    rows.append({\"class\":cat, **s, \"precision\":prec, \"recall\":rec, \"f1\":f1})\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"class\")\n",
    "df.to_csv(OUT_DIR/\"results.csv\", index=False)\n",
    "print(\"✓ Salvato results.csv in\", OUT_DIR)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# %% 6 – AGGREGATI\n",
    "macro_f1 = df[\"f1\"].mean()\n",
    "micro_tp = df.TP.sum(); micro_fp = df.FP.sum(); micro_fn = df.FN.sum()\n",
    "micro_p = micro_tp/(micro_tp+micro_fp+1e-9)\n",
    "micro_r = micro_tp/(micro_tp+micro_fn+1e-9)\n",
    "micro_f1 = 2*micro_p*micro_r/(micro_p+micro_r+1e-9)\n",
    "\n",
    "with open(OUT_DIR/\"overall_metrics.txt\",\"w\") as f:\n",
    "    f.write(f\"Macro-F1  : {macro_f1:.4f}\\n\");\n",
    "    f.write(f\"Micro-F1  : {micro_f1:.4f}\\n\");\n",
    "    f.write(f\"Micro-P   : {micro_p:.4f}\\n\");\n",
    "    f.write(f\"Micro-R   : {micro_r:.4f}\\n\");\n",
    "print((OUT_DIR/\"overall_metrics.txt\").read_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d911bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmwTeam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
